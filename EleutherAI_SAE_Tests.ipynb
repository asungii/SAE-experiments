{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/asungii/SAE-experiments/blob/main/EleutherAI_SAE_Tests.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2OmILXY6WwM7"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1gphulvjZptp"
      },
      "outputs": [],
      "source": [
        "%cd /content/gdrive/MyDrive/Colab Notebooks/sae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_FjXOvIXXsWI"
      },
      "outputs": [],
      "source": [
        "! ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "nV6xdFDRX42Y"
      },
      "outputs": [],
      "source": [
        "! pip install einops\n",
        "! pip install jaxtyping\n",
        "! pip install datasets\n",
        "! pip install simple_parsing\n",
        "! pip install accelerate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2X7-5ITAbJRV"
      },
      "outputs": [],
      "source": [
        "! pip install multiprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UnSTX7foj1sG"
      },
      "outputs": [],
      "source": [
        "! python -m sae"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qwFBD6v2Zfvl",
        "outputId": "49d95e75-c9ca-4756-9a5f-c53f2db506ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
            "Map (num_proc=4): 100% 927262/930514 [40:09<00:30, 107.96 examples/s]"
          ]
        }
      ],
      "source": [
        "! python -m sae EleutherAI/pythia-160m togethercomputer/RedPajama-Data-1T-Sample --attn_implementation=eager"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# For some reason I'm still getting the error even with the --attn_implementation=\"eager\"\n",
        "# oops i just didn't commit all the changes like an... idiot\n",
        "\n",
        "with open('/content/gdrive/MyDrive/Colab Notebooks/sae/sae/__main__.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "    for i in range(39, 80):\n",
        "        print(lines[i])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "StxLVv2LhNHH",
        "outputId": "b4e64c67-e422-47fc-ee43-fc6621d46d3b"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "    load_in_8bit: bool = False\n",
            "\n",
            "    \"\"\"Load the model in 8-bit mode.\"\"\"\n",
            "\n",
            "    \n",
            "\n",
            "    data_preprocessing_num_proc: int = field(\n",
            "\n",
            "        default_factory=lambda: cpu_count() // 2,\n",
            "\n",
            "    )\n",
            "\n",
            "    \"\"\"Number of processes to use for preprocessing data\"\"\"\n",
            "\n",
            "\n",
            "\n",
            "    attn_implementation: str = \"sdpa\"\n",
            "\n",
            "    \"\"\"Which implementation to use for attention in `transformers`. Pythia models require \"eager\".\"\"\"\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "def load_artifacts(args: RunConfig, rank: int) -> tuple[PreTrainedModel, Dataset]:\n",
            "\n",
            "    if args.load_in_8bit:\n",
            "\n",
            "        dtype = torch.float16\n",
            "\n",
            "    elif torch.cuda.is_bf16_supported():\n",
            "\n",
            "        dtype = torch.bfloat16\n",
            "\n",
            "    else:\n",
            "\n",
            "        dtype = \"auto\"\n",
            "\n",
            "\n",
            "\n",
            "    model = AutoModelForCausalLM.from_pretrained(\n",
            "\n",
            "        args.model,\n",
            "\n",
            "        attn_implementation=args.attn_implementation,\n",
            "\n",
            "        device_map={\"\": f\"cuda:{rank}\"},\n",
            "\n",
            "        quantization_config=(\n",
            "\n",
            "            BitsAndBytesConfig(load_in_8bit=args.load_in_8bit)\n",
            "\n",
            "            if args.load_in_8bit\n",
            "\n",
            "            else None\n",
            "\n",
            "        ),\n",
            "\n",
            "        torch_dtype=dtype,\n",
            "\n",
            "        token=args.hf_token,\n",
            "\n",
            "    )\n",
            "\n",
            "\n",
            "\n",
            "    dataset = load_dataset(\n",
            "\n",
            "        args.dataset,\n",
            "\n",
            "        split=args.split,\n",
            "\n",
            "        # TODO: Maybe set this to False by default? But RPJ requires it.\n",
            "\n",
            "        trust_remote_code=True,\n",
            "\n",
            "    )\n",
            "\n",
            "    tokenizer = AutoTokenizer.from_pretrained(args.model, token=args.hf_token)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJTRA97rhPbr"
      },
      "outputs": [],
      "source": [
        "# ykw maybe i'll try it programmatically\n",
        "\n",
        "# okay that seemed to work\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "\n",
        "from sae import SaeConfig, SaeTrainer, TrainConfig\n",
        "from sae.data import chunk_and_tokenize\n",
        "\n",
        "MODEL = \"EleutherAI/pythia-160m\"\n",
        "dataset = load_dataset(\n",
        "    \"togethercomputer/RedPajama-Data-1T-Sample\",\n",
        "    split=\"train\",\n",
        "    trust_remote_code=True,\n",
        ")\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, attn_implementation=\"eager\")\n",
        "tokenized = chunk_and_tokenize(dataset, tokenizer)\n",
        "\n",
        "\n",
        "gpt = AutoModelForCausalLM.from_pretrained(\n",
        "    MODEL,\n",
        "    device_map={\"\": \"cuda\"},\n",
        "    torch_dtype=torch.bfloat16,\n",
        ")\n",
        "\n",
        "cfg = TrainConfig(\n",
        "    SaeConfig(gpt.config.hidden_size), batch_size=16\n",
        ")\n",
        "trainer = SaeTrainer(cfg, tokenized, gpt)\n",
        "\n",
        "trainer.fit()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jy0oXZCbpXVA"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyNQb44U+iinrCUuQH1bvwzR",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}