{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"2OmILXY6WwM7"},"outputs":[],"source":["import os\n","\n","from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1gphulvjZptp"},"outputs":[],"source":["%cd /content/gdrive/MyDrive/Colab Notebooks/sae"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_FjXOvIXXsWI"},"outputs":[],"source":["! ls"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"nV6xdFDRX42Y"},"outputs":[],"source":["! pip install einops\n","! pip install jaxtyping\n","! pip install datasets\n","! pip install simple_parsing\n","! pip install accelerate"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2X7-5ITAbJRV"},"outputs":[],"source":["! pip install multiprocessing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UnSTX7foj1sG"},"outputs":[],"source":["! python -m sae"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qwFBD6v2Zfvl","outputId":"49d95e75-c9ca-4756-9a5f-c53f2db506ba"},"outputs":[{"output_type":"stream","name":"stdout","text":["Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","Map (num_proc=4): 100% 930514/930514 [40:22<00:00, 384.13 examples/s]\n","Training on 'togethercomputer/RedPajama-Data-1T-Sample' (split 'train')\n","Storing model weights in torch.bfloat16\n","Training on layers: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n","Auto-selected LR: 1.63e-04\n","bitsandbytes 8-bit Adam not available, using torch.optim.Adam\n","Run `pip install bitsandbytes` for less memory usage.\n","Weights & Biases not installed, skipping logging.\n","Number of SAE parameters: 453_288_960\n","Number of model parameters: 162_322_944\n","Training:   0% 25/73495 [04:29<216:14:23, 10.60s/it]"]}],"source":["! python -m sae EleutherAI/pythia-160m togethercomputer/RedPajama-Data-1T-Sample --attn_implementation=eager"]},{"cell_type":"code","source":["# For some reason I'm still getting the error even with the --attn_implementation=\"eager\"\n","# oops i just didn't commit all the changes like an... idiot\n","\n","with open('/content/gdrive/MyDrive/Colab Notebooks/sae/sae/__main__.py', 'r') as f:\n","    lines = f.readlines()\n","    for i in range(39, 80):\n","        print(lines[i])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"StxLVv2LhNHH","executionInfo":{"status":"ok","timestamp":1718852004320,"user_tz":300,"elapsed":1773,"user":{"displayName":"Laerdon Kim","userId":"10387999884776780758"}},"outputId":"b4e64c67-e422-47fc-ee43-fc6621d46d3b"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","\n","    load_in_8bit: bool = False\n","\n","    \"\"\"Load the model in 8-bit mode.\"\"\"\n","\n","    \n","\n","    data_preprocessing_num_proc: int = field(\n","\n","        default_factory=lambda: cpu_count() // 2,\n","\n","    )\n","\n","    \"\"\"Number of processes to use for preprocessing data\"\"\"\n","\n","\n","\n","    attn_implementation: str = \"sdpa\"\n","\n","    \"\"\"Which implementation to use for attention in `transformers`. Pythia models require \"eager\".\"\"\"\n","\n","\n","\n","\n","\n","def load_artifacts(args: RunConfig, rank: int) -> tuple[PreTrainedModel, Dataset]:\n","\n","    if args.load_in_8bit:\n","\n","        dtype = torch.float16\n","\n","    elif torch.cuda.is_bf16_supported():\n","\n","        dtype = torch.bfloat16\n","\n","    else:\n","\n","        dtype = \"auto\"\n","\n","\n","\n","    model = AutoModelForCausalLM.from_pretrained(\n","\n","        args.model,\n","\n","        attn_implementation=args.attn_implementation,\n","\n","        device_map={\"\": f\"cuda:{rank}\"},\n","\n","        quantization_config=(\n","\n","            BitsAndBytesConfig(load_in_8bit=args.load_in_8bit)\n","\n","            if args.load_in_8bit\n","\n","            else None\n","\n","        ),\n","\n","        torch_dtype=dtype,\n","\n","        token=args.hf_token,\n","\n","    )\n","\n","\n","\n","    dataset = load_dataset(\n","\n","        args.dataset,\n","\n","        split=args.split,\n","\n","        # TODO: Maybe set this to False by default? But RPJ requires it.\n","\n","        trust_remote_code=True,\n","\n","    )\n","\n","    tokenizer = AutoTokenizer.from_pretrained(args.model, token=args.hf_token)\n","\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hJTRA97rhPbr"},"outputs":[],"source":["# ykw maybe i'll try it programmatically\n","\n","# okay that seemed to work\n","\n","import torch\n","from datasets import load_dataset\n","from transformers import AutoModelForCausalLM, AutoTokenizer\n","\n","from sae import SaeConfig, SaeTrainer, TrainConfig\n","from sae.data import chunk_and_tokenize\n","\n","MODEL = \"EleutherAI/pythia-160m\"\n","dataset = load_dataset(\n","    \"togethercomputer/RedPajama-Data-1T-Sample\",\n","    split=\"train\",\n","    trust_remote_code=True,\n",")\n","tokenizer = AutoTokenizer.from_pretrained(MODEL, attn_implementation=\"eager\")\n","tokenized = chunk_and_tokenize(dataset, tokenizer)\n","\n","\n","gpt = AutoModelForCausalLM.from_pretrained(\n","    MODEL,\n","    device_map={\"\": \"cuda\"},\n","    torch_dtype=torch.bfloat16,\n",")\n","\n","cfg = TrainConfig(\n","    SaeConfig(gpt.config.hidden_size), batch_size=16\n",")\n","trainer = SaeTrainer(cfg, tokenized, gpt)\n","\n","trainer.fit()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jy0oXZCbpXVA"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","machine_shape":"hm","provenance":[],"authorship_tag":"ABX9TyNQb44U+iinrCUuQH1bvwzR"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}